{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project Goal\n",
    "\n",
    "The objective of this project is to perform an in-depth analysis of corporate annual reports from various companies over multiple years. The analysis is aimed at extracting, processing, and understanding the textual content within these reports to derive insights on:\n",
    "\n",
    "\n",
    "### Detailed Aims and Methodologies\n",
    "\n",
    "1. **In-depth Textual Data Extraction**\n",
    "   - The first phase of the project is dedicated to the sophisticated extraction of textual data from corporate annual reports, typically available in PDF format. This involves parsing document structures to convert visual data into analyzable text, setting the stage for all subsequent analytical endeavors.\n",
    "\n",
    "2. **Extraction and Analysis of R&D Sections**\n",
    "   - Recognizing the critical role of Research and Development (R&D) in corporate growth and innovation, this project includes a focused extraction and analysis of R&D sections from the annual reports. By pinpointing these sections based on the table of contents, the analysis seeks to delve into the companies' commitments to innovation, R&D spending trends, and the strategic importance of R&D activities in their overall business strategy.\n",
    "   \n",
    "3. **Multifaceted Sentiment Analysis**\n",
    "   - A significant emphasis is placed on dissecting the sentiment within the R&D annual reports. Employing a blend of traditional NLTK models and advanced transformer models, the project aims to capture a nuanced spectrum of sentiments, from explicit expressions to subtle tones that influence perceptions.\n",
    "\n",
    "4. **Granular Keyword Analysis**\n",
    "   - Delving into the specifics, the project conducts a detailed keyword analysis to identify and quantify the occurrence of specific positive and negative keywords within the reports. This granular analysis aims to gauge the reports' tonality, providing quantitative insights into the companies' narrative framing.\n",
    "\n",
    "5. **Thematic Exploration through Topic Modeling**\n",
    "   - Employing Latent Dirichlet Allocation (LDA), the project uncovers latent topics within the textual corpus, revealing the primary themes that pervade the corporate discourse. This thematic exploration is key to understanding the strategic priorities and operational challenges highlighted by companies.\n",
    "\n",
    "6. **Visual Representation through Word Clouds**\n",
    "   - To enhance data accessibility and comprehension, the project incorporates word clouds that visually highlight the most frequent words and themes, including separate visualizations for positive and negative keywords, offering an immediate sense of the reports' content and sentiment.\n",
    "\n",
    "7. **Thematic Consistency Analysis using Cosine Similarity**\n",
    "   - The project employs cosine similarity measures to assess thematic consistency or divergence across different years or among different companies. This analysis provides insights into the evolution or stability of strategic narratives over time in response to various factors.\n",
    "\n",
    "This project uses a suite of Python libraries, including `pandas` for data manipulation, `transformers` and `nltk` for natural language processing, `pdfplumber` for PDF text extraction, `gensim` for topic modeling, and `matplotlib` and `WordCloud` for visualization, along with custom sentiment analysis modules. The comprehensive analytical approach employed in this project enables stakeholders to extract actionable insights from annual reports, evaluating the evolution of company narratives and strategic focuses in alignment with market dynamics and expectations.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8579db2dd9132ba4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import requests\n",
    "import pdfplumber\n",
    "import concurrent.futures\n",
    "from io import BytesIO\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "import numpy as np\n",
    "from gensim.downloader import load\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This section of the code involves setting up the environment and defining functions for sentiment analysis using two approaches: the NLTK library and transformer models from the Hugging Face library.\n",
    "\n",
    "### Import Statements\n",
    "\n",
    "- `import sentiment_analysis.sentiment_nltk as n`: This imports NLTK-Based Sentiment Analysis Functions\n",
    "\n",
    "- `import sentiment_analysis.sentiment_transformers as t`: Imports Transformer Model-Based Sentiment Analysis Functions\n",
    "\n",
    "\n",
    "### NLTK-Based Sentiment Analysis Functions\n",
    "\n",
    "- `analyze_sentiment_vader(text)`: Utilizes the VADER tool from the NLTK library to calculate a compound sentiment score for the input text, reflecting the overall sentiment from negative (-1) to positive (+1).\n",
    "\n",
    "- `analyze_sentiment_vader_detail(text)`: Provides a detailed sentiment analysis using VADER, returning a dictionary with positive, negative, neutral, and compound scores.\n",
    "\n",
    "### Transformer Model-Based Sentiment Analysis Functions\n",
    "\n",
    "- `chunk_text(text, max_length)`: Splits a large text into smaller chunks of a specified maximum length. This is crucial for processing long texts with models that have input length limitations (max 512).\n",
    "\n",
    "- `analyze_sentiment(text)`: Performs sentiment analysis on the input text using a imported transformer model. It chunks the text if necessary and aggregates sentiment scores across chunks to derive an average score.\n",
    "\n",
    "- `analyze_sentiment(text, tokenizer, sentiment_analysis)`: An advanced version of the sentiment analysis function tailored for transformer models. It requires a tokenizer and a sentiment analysis \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19f881fae0fd9576"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Import custom modules\n",
    "import sentiment_analysis.sentiment_nltk as n\n",
    "import sentiment_analysis.sentiment_transformers as t"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e3ac7f054e0a005",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section, we define functions for downloading and processing annual reports from the website `annualreports.com`. The functions are designed to handle the following tasks directly within Python. (Note: there are predownloaded examples in the folder 'reports' as raw pdf files and in folder discussions as text files)\n",
    "\n",
    "### Function: `download_and_extract_text(company, year)`\n",
    "\n",
    "This function is responsible for downloading annual reports for a specified company and year. It constructs the URL based on the company name and year, handling a special case for reports other than 2022 year. For reports from other than 2022 year, it iterates through potential URL prefixes to locate the report. Upon successfully retrieving the report, it uses `pdfplumber` to extract text from the PDF file. The extracted text from all pages is concatenated and returned. The function also provides feedback on the success or failure of processing each report.\n",
    "\n",
    "### Function: `download_and_process_reports_parallel(companies, years)`\n",
    "\n",
    "This function orchestrates the parallel downloading and processing of annual reports for a list of companies and a range of years. It uses a `ThreadPoolExecutor` from the `concurrent.futures` module to manage concurrent execution, submitting tasks to download and extract text for each company-year combination. The results are aggregated in a dictionary, mapping each company-year pair to its corresponding extracted text, facilitating efficient parallel processing and reducing overall execution time.\n",
    "\n",
    "### Function: `get_top_n_topics(bow_doc, lda_model, n=5)`\n",
    "\n",
    "Given a document represented as a bag-of-words (BoW) and an LDA model, this function identifies the top N topics within the document. It retrieves the list of topics and their associated probabilities from the LDA model, sorts them by probability in descending order, and selects the top N topics.\n",
    "\n",
    "### Function: `get_top_keywords_for_topic(lda_model, topic_id, topn=5)`\n",
    "\n",
    "This function retrieves the top N keywords for a given topic within an LDA model.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5ad8d9f2dff4461"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def download_and_extract_text(company, year):\n",
    "    text = None\n",
    "    response = None\n",
    "\n",
    "    if year == '2022':\n",
    "        url = f'https://www.annualreports.com/HostedData/AnnualReports/PDF/{company}_{year}.pdf'\n",
    "        response = requests.get(url, stream=True)\n",
    "    else:\n",
    "        # This loop tries URLs with different prefixes for non-2022 reports\n",
    "        for prefix in [chr(97 + i) for i in range(26)]:  # Iterates through all lowercase letters (a to z)\n",
    "            url = f'https://www.annualreports.com/HostedData/AnnualReportArchive/{prefix}/{company}_{year}.pdf'\n",
    "            response = requests.get(url, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                break  # Exit the loop if a valid response is received\n",
    "\n",
    "    # Check if a valid response was received\n",
    "    if response is not None and response.status_code == 200:\n",
    "        with BytesIO(response.content) as bytes_io:\n",
    "            with pdfplumber.open(bytes_io) as pdf:\n",
    "                pages_text = [page.extract_text() for page in pdf.pages if page.extract_text() is not None]\n",
    "                text = ' '.join(pages_text)\n",
    "        print(f\"Successfully processed {company} {year}\")\n",
    "    else:\n",
    "        print(f\"Failed to process {company} {year}\")\n",
    "\n",
    "    return text\n",
    "\n",
    "def download_and_process_reports_parallel(companies, years):\n",
    "    all_texts = {}\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = {}\n",
    "        for company in companies:\n",
    "            for year in years:\n",
    "                future = executor.submit(download_and_extract_text, company, year)\n",
    "                futures[future] = (company, year)  # Map future to (company, year)\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            company, year = futures[future]  # Get company and year from the future\n",
    "            text = future.result()\n",
    "            if text:\n",
    "                all_texts[(company, year)] = text  # Store text with (company, year) as key\n",
    "\n",
    "    return all_texts\n",
    "\n",
    "def get_top_n_topics(bow_doc, lda_model, n=5):\n",
    "    # Get the list of topic probabilities for the document\n",
    "    doc_topics = lda_model.get_document_topics(bow_doc, minimum_probability=0.0)\n",
    "    \n",
    "    # Sort the topics by their probabilities (highest first)\n",
    "    sorted_doc_topics = sorted(doc_topics, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Select the top N topics\n",
    "    top_n_topics = sorted_doc_topics[:n]\n",
    "    \n",
    "    return top_n_topics\n",
    "\n",
    "def get_top_keywords_for_topic(lda_model, topic_id, topn=5):\n",
    "    top_keywords = lda_model.show_topic(topic_id, topn=topn)\n",
    "    return ', '.join([word for word, prob in top_keywords])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b733a78891e553e9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Function: `keyword_analysis(text)`\n",
    "\n",
    "This function performs a simple keyword analysis on the input text by counting the occurrences of predefined positive and negative keywords. It iterates through each list of keywords, tallying how many times each keyword appears within the text.\n",
    "\n",
    "### Function: `make_word_cloud(text, title=None)`\n",
    "\n",
    "This function generates a word cloud from the provided text, which is a visual representation highlighting the frequency of word occurrence in a visually appealing format. The function checks if the input text is non-empty and then proceeds to create a word cloud using the `WordCloud` class, with configurable dimensions and background color.\n",
    "\n",
    "### Function: `make_map(text, positive, negative)`\n",
    "\n",
    "The `make_map` function serves as a comprehensive visualization tool that generates word clouds for three categories: all words in the text, positive words, and negative words."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4c9f523f5ec11c7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def keyword_analysis(text):\n",
    "    positive_count = sum(text.count(word) for word in positive_keywords)\n",
    "    negative_count = sum(text.count(word) for word in negative_keywords)\n",
    "    return positive_count, negative_count\n",
    "\n",
    "def make_word_cloud(text, title=None):\n",
    "    if not text.strip():  # Checks if the text is empty or contains only whitespace\n",
    "        print(f\"No words found for '{title}'. Skipping word cloud generation.\")\n",
    "        return  # Exits the function early\n",
    "\n",
    "    # Proceed with word cloud generation if text is not empty\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def make_map(text, positive, negative):\n",
    "    # Display word cloud for all words\n",
    "    print(\"All Words Word Cloud:\")\n",
    "    make_word_cloud(text, \"All Words\")\n",
    "\n",
    "    # Display positive word cloud\n",
    "    print(\"Positive Word Cloud:\")\n",
    "    positive_text = ' '.join([word for word in text.lower().split() if word in positive])\n",
    "    make_word_cloud(positive_text, \"Positive Words\")\n",
    "\n",
    "    # Display negative word cloud\n",
    "    print(\"Negative Word Cloud:\")\n",
    "    negative_text = ' '.join([word for word in text.lower().split() if word in negative])\n",
    "    make_word_cloud(negative_text, \"Negative Words\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af755ba4060c52e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Function: `preprocess_text(text)`\n",
    "\n",
    "This function preprocesses the given text by removing English stopwords and tokenizing the text into individual words.\n",
    "\n",
    "### Function: `get_vector(word)`\n",
    "\n",
    "Attempts to retrieve the vector representation of a given word from a pre-trained word embedding model.\n",
    "\n",
    "### Function: `average_topic_vector(topic_keywords)`\n",
    "\n",
    "Calculates the centroid or average vector of a set of keywords representing a topic. This is achieved by averaging the vector representations of each keyword, resulting in a single vector that encapsulates the semantic essence of the entire topic.\n",
    "\n",
    "### Function: `cosine_similarity(vec1, vec2)`\n",
    "\n",
    "Computes the cosine similarity between two vectors.\n",
    "\n",
    "### Function: `calculate_cosine_similarities(company_name, df)`\n",
    "\n",
    "This function is designed to calculate and display a matrix of cosine similarities between topics associated with a specific company's reports across different years. It aggregates the topics for each year, computes their average vector representations, and then calculates the cosine similarity between each pair of yearly topic aggregations. The resulting matrix provides insights into the thematic consistency or evolution of the company's focus over time.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32135bc4a35737ab"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    return [word for word in simple_preprocess(text) if word not in stop_words]\n",
    "           \n",
    "def get_vector(word):\n",
    "    \"\"\"Return the vector for a given word if it exists in the model.\"\"\"\n",
    "    try:\n",
    "        return model[word]\n",
    "    except KeyError:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "def average_topic_vector(topic_keywords):\n",
    "    \"\"\"Compute the average vector for a list of topic keywords.\"\"\"\n",
    "    vectors = [get_vector(word) for word in topic_keywords.split(', ')]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2) if norm_vec1 > 0 and norm_vec2 > 0 else 0\n",
    "\n",
    "def calculate_cosine_similarities(company_name, df):\n",
    "    \"\"\"Calculate and print cosine similarities of topics aggregated by year for a given company.\"\"\"\n",
    "    # Filter the DataFrame for the specified company\n",
    "    company_df = df[df['Company'] == company_name]\n",
    "    yearly_topics = company_df.groupby('Year')['Topic Keywords'].apply(lambda topics: ', '.join(topics)).reset_index()\n",
    "    yearly_topics['Average Vector'] = yearly_topics['Topic Keywords'].apply(average_topic_vector)\n",
    "    cosine_similarities = pd.DataFrame(index=yearly_topics['Year'], columns=yearly_topics['Year'])\n",
    "    \n",
    "    # Calculate the cosine similarity between each pair of years\n",
    "    for i, row_i in yearly_topics.iterrows():\n",
    "        for j, row_j in yearly_topics.iterrows():\n",
    "            cosine_similarities.at[row_i['Year'], row_j['Year']] = cosine_similarity(row_i['Average Vector'], row_j['Average Vector'])\n",
    "    \n",
    "    # Print the cosine similarity matrix\n",
    "    print(cosine_similarities)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ecacc68bc89ff19",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This code snippet initializes the analysis by defining two key lists: `companies` and `years`. The `companies` list contains the stock symbols of the companies of interest, while the `years` list specifies the range of years for which the analysis will be conducted. The function `download_and_process_reports_parallel` is then invoked to concurrently download and extract text from the annual reports of these companies for the specified years. The resulting texts are stored in the `all_texts` dictionary, keyed by tuples of `(company, year)`.\n",
    "\n",
    "### Companies and Their Stock Symbols\n",
    "\n",
    "| Stock Symbol  | Company Name               |\n",
    "|---------------|----------------------------|\n",
    "| NASDAQ_TSLA   | Tesla, Inc.                |\n",
    "| NASDAQ_AAPL   | Apple Inc.                 |\n",
    "| NASDAQ_MSFT   | Microsoft Corporation      |\n",
    "| NASDAQ_AMZN   | Amazon.com, Inc.           |\n",
    "| NYSE_BRK-A    | Berkshire Hathaway Inc.    |\n",
    "| NYSE_PFE      | Pfizer Inc.                |\n",
    "| NASDAQ_CCBG   | Capital City Bank Group Inc.|\n",
    "\n",
    "Each entry in the `all_texts` dictionary represents the textual content extracted from an annual report for a specific company and year, providing a foundational dataset for subsequent analysis.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24bc363ccc97818c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the list of companies and years\n",
    "companies = ['NASDAQ_TSLA', 'NASDAQ_AAPL', 'NASDAQ_MSFT', 'NASDAQ_AMZN', 'NYSE_BRK-A', 'NYSE_PFE', 'NASDAQ_CCBG']\n",
    "years = ['2022', '2021', '2020', '2019', '2018']\n",
    "\n",
    "# Download and process the reports\n",
    "all_texts = download_and_process_reports_parallel(companies, years)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b4b1e244bc6c743",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This code segment sets up the foundational elements for sentiment analysis by defining lists of positive and negative keywords and initializing a pre-trained sentiment analysis model.\n",
    "\n",
    "- **Positive and Negative Keywords**: Two lists, `positive_keywords` and `negative_keywords`, are created to contain words commonly associated with positive and negative sentiments, respectively for financial reports.\n",
    "\n",
    "- **Sentiment Analysis Model**: The code initializes a sentiment analysis model using the `distilbert-base-uncased-finetuned-sst-2-english` model.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df25da21260f3631"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the positive and negative keywords\n",
    "positive_keywords = [\n",
    "    'good', 'great', 'positive', 'successful', 'profitable', 'improved', 'improving', 'excellent',\n",
    "    'beneficial', 'strong', 'growth', 'upturn', 'bullish', 'booming', 'advantageous',\n",
    "    'rewarding', 'lucrative', 'surplus', 'expansion', 'upswing', 'thriving', 'yielding',\n",
    "    'gains', 'outperform', 'optimistic', 'upbeat', 'recovery', 'acceleration', 'enhancement',\n",
    "    'rally', 'surge', 'boom', 'profitability', 'efficiency', 'superior', 'leadership',\n",
    "    'innovation', 'breakthrough', 'high-demand', 'competitive edge', 'market leader',\n",
    "    'dividend increase', 'shareholder value', 'capital gain', 'revenue growth', 'cost reduction',\n",
    "    'strategic acquisition', 'synergy', 'scalability', 'liquidity'\n",
    "]\n",
    "negative_keywords = [\n",
    "    'bad', 'poor', 'negative', 'loss', 'problem', 'decrease', 'difficult', 'weak', 'decline',\n",
    "    'losses', 'bearish', 'slump', 'downturn', 'adverse', 'challenging', 'deteriorating',\n",
    "    'declining', 'recession', 'deficit', 'contraction', 'downgrade', 'volatility', 'risk',\n",
    "    'uncertainty', 'impairment', 'write-off', 'underperform', 'pessimistic', 'downbeat',\n",
    "    'stagnation', 'erosion', 'turmoil', 'crisis', 'bankruptcy', 'default', 'devaluation',\n",
    "    'overleveraged', 'layoffs', 'restructuring', 'downsizing', 'liquidation', 'fraud',\n",
    "    'scandal', 'litigation', 'regulatory penalty', 'market exit', 'competitive pressure',\n",
    "    'product recall', 'safety concern'\n",
    "]\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=model_name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "848f20f25928c2a9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This code segment performs an in-depth analysis of textual content from annual reports for specified companies and years, focusing on sentiment analysis, keyword frequency, and topic modeling.\n",
    "\n",
    "**Analysis Loop**: Iterates over each `(company, year)` pair, conducting analyses if the text for the pair is available in `all_texts`.\n",
    "\n",
    "**Sentiment Analysis**: \n",
    "   - Utilizes VADER from NLTK to compute sentiment scores.\n",
    "   - Employs a transformer-based model for a detailed sentiment analysis, displaying scores for each `(company, year)` pair.\n",
    "\n",
    "**Keyword Analysis**: \n",
    "   - Cleans the text and counts occurrences of predefined positive and negative keywords.\n",
    "   - Calculates the ratio of positive to negative keywords and visualizes them using word clouds.\n",
    "\n",
    "**Topic Modeling**: \n",
    "   - Preprocesses the text to remove stopwords and tokenizes it.\n",
    "   - Constructs a dictionary and corpus for LDA, then trains an LDA model to identify main topics and extracts top keywords for these topics.\n",
    "\n",
    "**Data Aggregation**: \n",
    "   - Compiles sentiment scores, keyword counts, and topic keywords into a dictionary for each company-year pair.\n",
    "   - Appends each dictionary to a list for later conversion into a DataFrame.\n",
    "\n",
    "**DataFrame Creation**: \n",
    "   - Converts the list of dictionaries into a pandas DataFrame, providing a structured overview of analysis results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b510db48a974bcb3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = []\n",
    "lda_models = {}\n",
    "text_key = [(company, year) for company in companies for year in years]\n",
    "\n",
    "for key in text_key:\n",
    "    company, year = key\n",
    "    if key in all_texts:\n",
    "        text = all_texts[key]\n",
    "        sentiment_score_nltk = n.analyze_sentiment_vader(text)\n",
    "        print(f\"{company} {year}: Keyword Analysis\")\n",
    "        print(f\"\\n Sentiment Score NLTK = {sentiment_score_nltk}\")\n",
    "        scores = n.analyze_sentiment_vader_detail(text)\n",
    "        print(f\"  Positive Score: {scores['pos']}\")\n",
    "        print(f\"  Negative Score: {scores['neg']}\")\n",
    "        print(f\"  Neutral Score: {scores['neu']}\")\n",
    "\n",
    "        cleaned_text = t.clean_text(text)\n",
    "        sentiment_score_transformer = t.analyze_sentiment(cleaned_text, tokenizer, sentiment_analysis)\n",
    "        print(f\"{company} {year}: Sentiment Score Transformer = {sentiment_score_transformer}\\n\")\n",
    "\n",
    "        # Keyword analysis\n",
    "        cleaned_text = t.clean_text(text)\n",
    "        positive_count, negative_count = keyword_analysis(cleaned_text)\n",
    "\n",
    "        print('Positive Keywords:', positive_count)\n",
    "        print('Negative Keywords:', negative_count)\n",
    "        print('Positive Keywords Ratio:', positive_count / (negative_count + positive_count ) if (negative_count + positive_count ) != 0 else \"N/A\")\n",
    "        make_map(text, positive_keywords, negative_keywords)\n",
    "        \n",
    "        # topic modeling part \n",
    "        processed_text = preprocess_text(text)\n",
    "        id2word = corpora.Dictionary([processed_text])\n",
    "        corpus = [id2word.doc2bow(processed_text)]\n",
    "        lda_model = LdaModel(corpus=corpus, id2word=id2word, num_topics=10,\n",
    "                     alpha='asymmetric',\n",
    "                     eta='auto',\n",
    "                     iterations=400,\n",
    "                     passes=15,\n",
    "                     eval_every=None)\n",
    "        lda_models[(company, year)] = lda_model\n",
    "        bow_text = id2word.doc2bow(processed_text)\n",
    "        top_topics = get_top_n_topics(bow_text, lda_model, n=1)\n",
    "\n",
    "        # Extract the top keyword for each of the top 5 topics\n",
    "        top_keywords = [get_top_keywords_for_topic(lda_model, topic_id) for topic_id, _ in top_topics]\n",
    "\n",
    "        # Append data with top keywords as separate columns\n",
    "        data.append({\n",
    "            \"Company\": company,\n",
    "            \"Year\": year,\n",
    "            \"NLTK_Sentiment_Score\": sentiment_score_nltk,\n",
    "            \"Positive_Score\": scores['pos'],\n",
    "            \"Negative_Score\": scores['neg'],\n",
    "            \"Neutral_Score\": scores['neu'],\n",
    "            \"Transformer_Sentiment_Score\": sentiment_score_transformer,\n",
    "            \"Positive_Keywords\": positive_count,\n",
    "            \"Negative_Keywords\": negative_count,\n",
    "            \"Positive_Keywords_Ratio\": positive_count / (negative_count + positive_count ) if (negative_count + positive_count ) != 0 else \"N/A\",\n",
    "            **{f\"Topic Keywords\": keyword for i, keyword in enumerate(top_keywords)}\n",
    "        })\n",
    "        print('-' * 50,'\\n')\n",
    "    \n",
    "df = pd.DataFrame(data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c28489704bafed25",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results of the analysis are stored in a pandas DataFrame. The DataFrame contains the following columns: `Company`, `Year`, `NLTK_Sentiment_Score`, `Positive_Score`, `Negative_Score`, `Neutral_Score`, `Transformer_Sentiment_Score`, `Positive_Keywords`, `Negative_Keywords`, `Positive_Keywords_Ratio`, and `Topic Keywords`. The DataFrame is displayed below."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c8e7d7dd1f9ca23"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1216e0e6aecc56de",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This segment provides results of the topic modeling analysis as plot."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5c9439d802baba4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "key = ('NASDAQ_TSLA', '2022')\n",
    "model = lda_models[key]\n",
    "corpus = [id2word.doc2bow(preprocess_text(all_texts[key]))]\n",
    "# vis = gensimvis.prepare(model, corpus, id2word)\n",
    "# pyLDAvis.display(vis)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b417eaa5ad9fe3c4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "This section of the code is focused on loading a pre-trained word embedding model and performing cosine similarity calculations to analyze the thematic consistency or divergence of topics associated with companies' annual reports over different years. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba40cbe64d67a004"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = load('glove-wiki-gigaword-100')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6589a72984dc7a7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "calculate_cosine_similarities('NASDAQ_TSLA', df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4143f13eb6b6cda8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for company in companies:\n",
    "    print(f\"\\n\\nCompany: {company}\")\n",
    "    company_df = df[df['Company'] == company]\n",
    "    company_topics = {}\n",
    "    company_df = company_df[['Year', 'Topic Keywords']]\n",
    "    print(company_df)\n",
    "    calculate_cosine_similarities(company, df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b73e59baa35657d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f60f2dd56a37199",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
